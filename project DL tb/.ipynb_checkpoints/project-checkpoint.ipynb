{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "38e4f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from dnn_app_utils_v4 import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fb4bb6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(918, 12)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('heart_convert.npy')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1d7aa4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, :11]\n",
    "Y = data[:, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "09c0da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=123)\n",
    "Y_train = np.expand_dims(y_train,axis=1)\n",
    "Y_test = np.expand_dims(y_test,axis=1)\n",
    "X_train, X_test, Y_train, Y_test = X_train.T, X_test.T, Y_train.T, Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "67cf9cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X:  (11, 734)\n",
      "Training Y:  (1, 734)\n",
      "Testing X:  (11, 184)\n",
      "Testing Y:  (1, 184)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training X: \",X_train.shape)\n",
    "print(\"Training Y: \",Y_train.shape)\n",
    "print(\"Testing X: \",X_test.shape)\n",
    "print(\"Testing Y: \",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1393a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(layers_dims):\n",
    "    np.random.seed(3)\n",
    "    L = len(layers_dims)\n",
    "    parameters = {}\n",
    "    for l in range(1,L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(2/layers_dims[l-1])\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layers_dims[l],1))\n",
    "    return parameters\n",
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    return A,Z\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    return A,Z\n",
    "def calc_ffw(W,A_prev,b,activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z = np.dot(W,A_prev) + b\n",
    "        A,Z = sigmoid(Z)\n",
    "    else:\n",
    "        Z = np.dot(W,A_prev) + b\n",
    "        A,Z = relu(Z)\n",
    "    linear_cache = (A_prev,W,b)\n",
    "    activation_cache = Z\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A,cache\n",
    "def L_model_forward(X,parameters):\n",
    "    L = len(parameters)//2\n",
    "    caches = []\n",
    "    A = X\n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        A,cache = calc_ffw(parameters[\"W\"+str(l)],A_prev,parameters[\"b\"+str(l)],\"relu\")\n",
    "        caches.append(cache)\n",
    "    AL,cache = calc_ffw(parameters[\"W\"+str(L)],A,parameters[\"b\"+str(L)],\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    return AL, caches\n",
    "def compute_cost(AL,Y,parameters,lamb):\n",
    "    m = AL.shape[1]\n",
    "    cross_entropy_cost = -1./m*(np.dot(Y,np.log(AL).T) + np.dot((1-Y),np.log(1-AL).T))\n",
    "    sum_W = 0\n",
    "    for l in range(1,len(parameters)//2+1):\n",
    "        sum_W += np.sum(np.square(parameters[\"W\"+str(l)]))\n",
    "    L2_regularization_cost =  sum_W* 1./m * lamb/2\n",
    "    cost = np.squeeze(cross_entropy_cost + L2_regularization_cost)  \n",
    "    return cost\n",
    "def sigmoid_grad(dA,Z):\n",
    "    A,Z = sigmoid(Z)\n",
    "    dZ = dA*A*(1-A)\n",
    "    return dZ\n",
    "def relu_grad(dA,Z):\n",
    "    A,Z = relu(Z)\n",
    "    dZ = np.multiply(dA, np.int64(A > 0))\n",
    "    return dZ\n",
    "def linear_backward(dZ,cache,lamb):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1] \n",
    "    dW = 1./m*np.dot(dZ,A_prev.T) + lamb/m*W\n",
    "    db = 1./m*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    return dA_prev,dW,db\n",
    "def linear_activation_backward(dA,cache,activation,lamb):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_grad(dA,activation_cache)\n",
    "        dA_prev,dW,db = linear_backward(dZ,linear_cache,lamb)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_grad(dA,activation_cache)\n",
    "        dA_prev,dW,db = linear_backward(dZ,linear_cache,lamb)\n",
    "    return dA_prev,dW,db\n",
    "def L_model_backward(AL, Y, caches,lamb):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    dAL = np.divide(AL - Y, np.multiply(AL, 1 - AL))\n",
    "\n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL,current_cache, activation = \"sigmoid\",lamb=lamb)\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)],  current_cache, activation = \"relu\", lamb=lamb)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    return grads\n",
    "def update_parameters(parameters, grads, lr):\n",
    "    L = len(parameters)//2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-lr*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-lr*grads[\"db\" + str(l+1)]\n",
    "    return parameters\n",
    "def predict(X,Y,parameters):\n",
    "    AL,cache =  L_model_forward(X,parameters)\n",
    "    return np.round(AL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a1f06abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,Y,X_test,y_test,num_iterations, learning_rate, lamb,print_cost=False):\n",
    "    np.random.seed(3)\n",
    "    input_size = X.shape[0]\n",
    "    layer_dims = []\n",
    "    layer_dims.append(input_size)\n",
    "    layer_dims.append(16)\n",
    "    #layer_dims.append(8)\n",
    "    layer_dims.append(4)\n",
    "    layer_dims.append(1)\n",
    "    parameters = init_parameters(layer_dims)\n",
    "    #return parameters\n",
    "    costs = []\n",
    "    for i in range(num_iterations):\n",
    "        AL, caches = L_model_forward(X,parameters)\n",
    "        cost = compute_cost(AL, Y,parameters,lamb=lamb)\n",
    "        costs.append(cost)\n",
    "        grads = L_model_backward(AL,Y,caches,lamb=lamb)\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    Y_train_pred = predict(X,Y,parameters)\n",
    "    Y_test_pred = predict(X_test,y_test,parameters)\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_train_pred - Y)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_test_pred - y_test)) * 100))\n",
    "    return parameters,costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "926ea121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train.shape[0]\n",
    "input_size\n",
    "print(Y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2f0e4367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\84765\\AppData\\Local\\Temp/ipykernel_1696/1925810604.py:39: RuntimeWarning: divide by zero encountered in log\n",
      "  cross_entropy_cost = -1./m*(np.dot(Y,np.log(AL).T) + np.dot((1-Y),np.log(1-AL).T))\n",
      "C:\\Users\\84765\\AppData\\Local\\Temp/ipykernel_1696/1925810604.py:74: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dAL = np.divide(AL - Y, np.multiply(AL, 1 - AL))\n",
      "C:\\Users\\84765\\AppData\\Local\\Temp/ipykernel_1696/1925810604.py:74: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dAL = np.divide(AL - Y, np.multiply(AL, 1 - AL))\n",
      "C:\\Users\\84765\\AppData\\Local\\Temp/ipykernel_1696/1925810604.py:48: RuntimeWarning: invalid value encountered in multiply\n",
      "  dZ = dA*A*(1-A)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: nan\n",
      "Cost after iteration 1000: nan\n",
      "Cost after iteration 2000: nan\n",
      "Cost after iteration 3000: nan\n",
      "Cost after iteration 4000: nan\n",
      "Cost after iteration 5000: nan\n",
      "Cost after iteration 6000: nan\n",
      "Cost after iteration 7000: nan\n",
      "Cost after iteration 8000: nan\n",
      "Cost after iteration 9000: nan\n",
      "Cost after iteration 10000: nan\n",
      "Cost after iteration 11000: nan\n",
      "Cost after iteration 12000: nan\n",
      "Cost after iteration 13000: nan\n",
      "Cost after iteration 14000: nan\n",
      "train accuracy: nan %\n",
      "test accuracy: nan %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters,costs = model(X_train,Y_train,X_test,Y_test,15000,0.01,0.9,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e418fa62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1696/3884004263.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b187ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
